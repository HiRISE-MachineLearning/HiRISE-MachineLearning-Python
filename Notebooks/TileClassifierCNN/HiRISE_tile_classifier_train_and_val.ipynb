{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script uses over 300 GB of RAM\n",
    "\n",
    "#Tbis notebook loads data and then uses leave-one-region-out cross-validation, to train 28 models, \n",
    "#doing three repeats of each\n",
    "#Each model takes about 10 minutes to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before starting: update base folder name\n",
    "base_folder = '../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#select a GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../SemanticSegmenterCNN') # for P4_DataHandlers\n",
    "\n",
    "#imports\n",
    "from p4tools import io # from https://github.com/michaelaye/p4tools: \n",
    "from P4_DataHandlers import Load_P4_Data_PreSaved,CreateFullImageMasks,CreateListOfThreeScales,Load_all_HiRISE_images_crop_and_convert_to_8bits\n",
    "\n",
    "#keras and tensorflow imports needed for specifying and training a model\n",
    "import tensorflow\n",
    "print(\"tensorflow version = \",tensorflow.__version__)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import Input,GlobalAveragePooling2D,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier parameters\n",
    "DoScaleAug = True #set this to false unless you have 400 GB of RAM!\n",
    "num_classes = 2\n",
    "init_lr = 0.001\n",
    "num_epochs = 10\n",
    "FirstEpochStep=9\n",
    "SecondEpochStep=20\n",
    "batch_size = 16\n",
    "My_wd = 5e-4\n",
    "NumRepeats=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get meta data information\n",
    "Tiles_df = io.get_tile_coordinates()\n",
    "print(\"Number of HiRise full Images = \", Tiles_df['obsid'].unique().shape[0])\n",
    "\n",
    "#regions\n",
    "region_names_df = io.get_region_names()\n",
    "#add some missing region names\n",
    "region_names_df = region_names_df.set_index('obsid')\n",
    "region_names_df.at['ESP_012620_0975','roi_name'] = 'Buffalo'\n",
    "region_names_df.at['ESP_012277_0975','roi_name'] = 'Buffalo'\n",
    "region_names_df.at['ESP_012348_0975','roi_name'] = 'Taichung'\n",
    "\n",
    "#metadata\n",
    "metadata_df = io.get_meta_data()\n",
    "for index, row in metadata_df.iterrows():\n",
    "    roi_name = region_names_df.at[row['OBSERVATION_ID'],'roi_name']\n",
    "    metadata_df.at[index,'roi_name']=roi_name\n",
    "    \n",
    "#ensure we use only \n",
    "ImageResults_df = io.get_meta_data()\n",
    "ImageResults_df = ImageResults_df.set_index('OBSERVATION_ID')\n",
    "ImageResults_df = pd.concat([ImageResults_df, region_names_df], axis=1, sort=False)\n",
    "ImageResults_df=ImageResults_df.dropna()\n",
    "\n",
    "UniqueP4Regions = ImageResults_df['roi_name'].unique()\n",
    "UniqueP4Regions = metadata_df['roi_name'].unique()\n",
    "print(\"Number of P4 regions = \",len(UniqueP4Regions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for training models (create 3 scales for each, to speed up training. Uses lots of RAM)\n",
    "t0 = time.time()\n",
    "ImageAndMasksList=[]\n",
    "OBSID_List=[]\n",
    "DataFolder = base_folder+'/Data/Images/HiRISE_8bit_and_P4_mask/'\n",
    "\n",
    "Preload=False\n",
    "if Preload==False:\n",
    "    #load all the images and convert from 10 bits maximum bit depth per channel to 8. Then crop to the labelled region\n",
    "    #Takes about 70 GB of RAM\n",
    "    t0 = time.time()\n",
    "    ImageAndMasksList,OBSID_List = Load_all_HiRISE_images_crop_and_convert_to_8bits(base_folder+'Data/Images/HiRISE/') #result has 3 channels (RGB)\n",
    "    t1 = time.time()\n",
    "    print('Number of images loaded = ',len(ImageAndMasksList))\n",
    "    print(\"time to load and convert all images = \",t1-t0) #takes about 2 hours\n",
    "    #step 3: create \"ground truth\" segmentation masks.\n",
    "    t0 = time.time()\n",
    "    ImageAndMasksList =  CreateFullImageMasks(ImageAndMasksList) # Result has 4 channels ( RGB and the 4th dimension in the channels axis is the masks)\n",
    "    t1 = time.time()\n",
    "    print(\"time to create all masks = \",t1-t0)#takes 2-3 hours\n",
    "    if not os.path.exists(DataFolder):\n",
    "        os.makedirs(DataFolder)  \n",
    "    for i in range(len(ImageAndMasksList)):\n",
    "        obs_id = OBSID_List[i]\n",
    "        np.save(DataFolder+obs_id+'.npy', ImageAndMasksList[i])\n",
    "else:\n",
    "    #assumes the first if branch above has already been run\n",
    "    for index, row in metadata_df.iterrows():\n",
    "        OBSID_List.append( row['OBSERVATION_ID'])\n",
    "        ImageAndMasksList.append(np.load(DataFolder+row['OBSERVATION_ID']+'.npy'))\n",
    "    t1 = time.time()\n",
    "    print(\"time to load all presaved images = \",t1-t0) \n",
    "    print('Number of images loaded = ',len(ImageAndMasksList))\n",
    "    \n",
    "assert len(ImageAndMasksList)==Tiles_df['obsid'].unique().shape[0]\n",
    "\n",
    "if DoScaleAug == True:\n",
    "    t0 = time.time()\n",
    "    ImageAndMasksLists = CreateListOfThreeScales(ImageAndMasksList) #takes 1.5-2 hours\n",
    "    t1 = time.time()\n",
    "    print('time to createlist of three scales for scale augmentation = ',t1-t0) \n",
    "else:\n",
    "    ImageAndMasksLists=[ImageAndMasksList]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_balanced_accuracy_score(y_true, y_pred):\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import backend as K\n",
    "    return tf.numpy_function(balanced_accuracy_score, (K.argmax(y_true,axis=-1), K.argmax(y_pred,axis=-1)), tf.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P4_TileClassifierDataGenerator(Sequence):\n",
    "    def __init__(self, ImageLists,batch_size,patch_height,patch_width):\n",
    "        self.ImageLists = ImageLists #this is a list of lists of training images at different scales\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_height = patch_height\n",
    "        self.patch_width = patch_width\n",
    "        self.ordering = np.arange(len(self.ImageLists[0]))\n",
    "        np.random.shuffle(self.ordering) \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.ordering)\n",
    "    def __len__(self):\n",
    "        return 35*int(np.floor(len(self.ImageLists[0])/self.batch_size)) #35 is in the order of the number of tiles per image\n",
    "    def __getitem__(self, index):\n",
    "        #the data coming in has 4 channels (RGB plus mask). Do augmentation first and only separate for return\n",
    "        imgs=[]\n",
    "        labels=[]\n",
    "        for i in range(self.batch_size):\n",
    "            #randomly choose from the scales available in the list of image lists\n",
    "            WhichScale=np.random.randint(len(self.ImageLists))            \n",
    "            img=self.ImageLists[WhichScale][self.ordering[(index*self.batch_size+i)%35]]\n",
    "            #get a random location within the image\n",
    "            start_height=np.random.randint(img.shape[0]-self.patch_height)\n",
    "            start_width=np.random.randint(img.shape[1]-self.patch_width)\n",
    "            #get the patch\n",
    "            ThisPatch=copy.deepcopy(img[start_height:start_height+self.patch_height,start_width:start_width+self.patch_width,:])\n",
    "            #add random flips and 90 degree rotations\n",
    "            if np.random.rand(1)>0.5:\n",
    "                ThisPatch=np.flipud(ThisPatch)\n",
    "            if np.random.rand(1)>0.5:\n",
    "                ThisPatch=np.fliplr(ThisPatch)\n",
    "            if np.random.rand(1)>0.5:\n",
    "                ThisPatch=np.rot90(ThisPatch)\n",
    "            imgs.append(ThisPatch)\n",
    "            labels.append(np.amax(ThisPatch[:,:,-1])>0)\n",
    "        #convert list to numpy\n",
    "        imgs=np.stack(imgs,axis=0)\n",
    "        labels=np.array(labels)\n",
    "        #return the image channels and categorical mask representations separately\n",
    "        return imgs[:,:,:,0:3],tensorflow.keras.utils.to_categorical(labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    lr = init_lr\n",
    "    if epoch >= SecondEpochStep:\n",
    "        lr *= 1e-2\n",
    "    elif epoch >= FirstEpochStep:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "    \n",
    "#loop over each region, and leave it out of training for a model. Use the left out region for val\n",
    "UniqueP4Regions = metadata_df['roi_name'].unique()\n",
    "\n",
    "ModelsPath='../../Data/Models/TileClassifier/'\n",
    "if not os.path.isdir(ModelsPath):\n",
    "    os.mkdir(ModelsPath)\n",
    "\n",
    "for Repeat in range(NumRepeats):\n",
    "    ModelCount = 0\n",
    "    for ToLeaveOut in UniqueP4Regions:\n",
    "        #select data for training\n",
    "        LeaveOneRegionOutList = [n for n in UniqueP4Regions if n != ToLeaveOut]\n",
    "        print('Training model ',str(ModelCount+1),' of ',str(len(UniqueP4Regions)),'; leaving out region: ',ToLeaveOut)\n",
    "        TrainImages_df = metadata_df[metadata_df['roi_name'] != ToLeaveOut]\n",
    "        if DoScaleAug==True:\n",
    "            TrainImagesList1 = [ImageAndMasksLists[0][i] for i in  TrainImages_df.index.values]\n",
    "            TrainImagesList2 = [ImageAndMasksLists[1][i] for i in  TrainImages_df.index.values]\n",
    "            TrainImagesList3 = [ImageAndMasksLists[2][i] for i in  TrainImages_df.index.values]\n",
    "            TrainImagesList = [TrainImagesList1,TrainImagesList2,TrainImagesList3]\n",
    "        else:\n",
    "            TrainImagesList = [[ImageAndMasksLists[0][i] for i in  TrainImages_df.index.values]]\n",
    "        train_data_generator = P4_TileClassifierDataGenerator(TrainImagesList,batch_size,384,384)\n",
    "        #define the model - we use transfer learning, based on a ResNet50 pretrained on imagenet\n",
    "        base_model = ResNet50(include_top=False, weights='imagenet')\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        predictions = Dense(num_classes, activation='softmax',\n",
    "                        kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(My_wd),use_bias=False)(x)\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer =SGD(lr=init_lr, decay=0, momentum=0.9, nesterov=False),\n",
    "                  metrics=['accuracy',metric_balanced_accuracy_score])\n",
    "        #fit the model\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        history = model.fit_generator(train_data_generator,\n",
    "                                      epochs=num_epochs,\n",
    "                                      verbose=1, \n",
    "                                      workers=8,\n",
    "                                      max_queue_size=50,\n",
    "                                      callbacks=[lr_scheduler]\n",
    "                                     )\n",
    "        model.save(ModelsPath+'HiRISE_tile_classifier_leave_out_' + ToLeaveOut+ '_final_repeat'+str(Repeat)+'.h5') \n",
    "        ModelCount=ModelCount+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "ResultsPath='../../Data/SummaryResults/'\n",
    "if not os.path.isdir(ResultsPath):\n",
    "    os.mkdir(ResultsPath)\n",
    "\n",
    "for Repeat in range(NumRepeats):\n",
    "    ModelCount = 0\n",
    "    Results_df = pd.DataFrame()\n",
    "    for ToLeaveOut in UniqueP4Regions:\n",
    "        LeaveOneRegionOutList = [n for n in UniqueP4Regions if n != ToLeaveOut]\n",
    "        print('model ',str(ModelCount+1),' of ',str(len(UniqueP4Regions)),'; validation region: ',ToLeaveOut)\n",
    "        ValImages_df = metadata_df[metadata_df['roi_name'] == ToLeaveOut]\n",
    "        ValImagesList=[]\n",
    "        for i in range(ValImages_df.shape[0]):\n",
    "            if ValImages_df.iloc[i]['map_scale']==0.25:\n",
    "                ScaleInd=2\n",
    "            elif ValImages_df.iloc[i]['map_scale']==0.5:\n",
    "                ScaleInd=1\n",
    "            else:\n",
    "                ScaleInd=0\n",
    "            ValImagesList.append(ImageAndMasksLists[ScaleInd][ValImages_df.index[i]])\n",
    "        model.load_weights(ModelsPath+'HiRISE_tile_classifier_leave_out_' + ToLeaveOut+ '_final_repeat'+str(Repeat)+'.h5') \n",
    "        CC=0\n",
    "        for Im in ValImages_df['OBSERVATION_ID']:\n",
    "            ThisImageTiles = Tiles_df[Tiles_df['obsid']==Im]\n",
    "            ThisImageTiles=ThisImageTiles.reset_index(drop=True)\n",
    "            ThisImageTiles['y_hirise']=ThisImageTiles['y_hirise'].astype('uint32')\n",
    "            ThisImageTiles['x_hirise']=ThisImageTiles['x_hirise'].astype('uint32')\n",
    "            FullIm=ValImagesList[CC][:,:,0:3]\n",
    "            FullMask=ValImagesList[CC][:,:,3]\n",
    "            Tiles=[]\n",
    "            for i in range(ThisImageTiles.shape[0]):\n",
    "                Results_df.at[Im+'_'+str(i),'tile_id']= ThisImageTiles.at[i,'tile_id']\n",
    "                Results_df.at[Im+'_'+str(i),'Region']=ToLeaveOut\n",
    "                Start_col =int(ThisImageTiles.at[i,'x_hirise']-420)\n",
    "                Start_row =int(ThisImageTiles.at[i,'y_hirise']-324)\n",
    "                TileImage=FullIm[Start_row:min(Start_row+648,FullIm.shape[0]),Start_col:min(Start_col+840,FullIm.shape[1]),:]\n",
    "                TileMask = FullMask[Start_row:min(Start_row+648,FullIm.shape[0]),Start_col:min(Start_col+840,FullIm.shape[1])]\n",
    "                Results_df.at[Im+'_'+str(i),'Region']=ToLeaveOut\n",
    "                Results_df.at[Im+'_'+str(i),'GroundTruth']=np.amax(TileMask)\n",
    "                if TileImage.shape[0] != 648 or TileImage.shape[1] != 840:\n",
    "                    aaa=255*np.ones((648,840,3),'uint8')\n",
    "                    aaa[0:TileImage.shape[0],0:TileImage.shape[1],:]=TileImage\n",
    "                    TileImage=aaa\n",
    "                Tiles.append(TileImage)\n",
    "            Tiles=np.stack(Tiles,axis=0)\n",
    "            PredClass = model.predict(Tiles)\n",
    "            for i in range(ThisImageTiles.shape[0]):\n",
    "                Results_df.at[Im+'_'+str(i),'ClassifierConf']=PredClass[i,1]\n",
    "            CC+=1\n",
    "        ModelCount=ModelCount+1\n",
    "    Results_df.to_csv(ResultsPath+'TileClassifier_LORO_final_repeat'+str(Repeat)+'.csv')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
