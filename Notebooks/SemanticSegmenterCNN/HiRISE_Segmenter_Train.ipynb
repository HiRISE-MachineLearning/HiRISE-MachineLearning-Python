{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script uses over 300 GB of RAM\n",
    "\n",
    "#loading and setting up the data (if already downloaded) takes over 6 hours\n",
    "#We then use leave-one-region-out cross-validation, and train 28 models\n",
    "#Each model takes about 10 hours to train using 1 gpu\n",
    "\n",
    "#overall runtime for 1 gpu= 6 + 28*10 = 286 = 12 days\n",
    "#overall runtime for 4 gpu=s 6 + 28*3 = 90 ~ 4 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE!\n",
    "#this notebook assumes Notebooks/DownloadHiRISE/DownloadHiRISE.ipynb has already been run to download the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, History\n",
    "print(\"tensorflow version = \",tensorflow.__version__)\n",
    "\n",
    "from p4tools import io #from https://github.com/michaelaye/p4tools \n",
    "from P4_DataHandlers import Load_all_HiRISE_images_crop_and_convert_to_8bits,CreateListOfThreeScales,CreateFullImageMasks\n",
    "from SemanticSegmentation import model_unet_P4,model_hrnet_P4,P4_SegmenterDataGenerator\n",
    "from SemanticSegmentation import loss_jaccard_coef_categorical_one_minus,metric_recall,metric_precision,metric_jaccard_coef_categorical_int\n",
    "\n",
    "#select a GPU and set up multi-GPU training\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tensorflow.config.experimental.set_memory_growth(gpu, True)\n",
    "strategy = tensorflow.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "#recommended path settings:\n",
    "base_folder = '../../'\n",
    "data_folder = base_folder+'Data/Images/HiRISE/'\n",
    "DoScaleAug = True #set this to false unless you have 400 GB of RAM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveFolder = base_folder+'/Data/Images/HiRISE_8bit_and_P4_mask/'\n",
    "Preload=True\n",
    "if Preload==False:\n",
    "    #load all the images and convert from 10 bits maximum bit depth per channel to 8. Then crop to the labelled region\n",
    "    #Takes about 70 GB of RAM\n",
    "    t0 = time.time()\n",
    "    ImageAndMasksList,OBSID_List = Load_all_HiRISE_images_crop_and_convert_to_8bits(data_folder) #result has 3 channels (RGB)\n",
    "    t1 = time.time()\n",
    "    print('Number of images loaded = ',len(ImageAndMasksList))\n",
    "    print('time to load and convert all images = ',t1-t0) #takes about 2 hours\n",
    "    #create \"ground truth\" segmentation masks.\n",
    "    t0 = time.time()\n",
    "    ImageAndMasksList =  CreateFullImageMasks(ImageAndMasksList) # Result has 4 channels ( RGB and the 4th dimension in the channels axis is the masks)\n",
    "    t1 = time.time()\n",
    "    print('time to create all masks = ',t1-t0)#takes 2-3 hours\n",
    "    if not os.path.exists(SaveFolder):\n",
    "        os.makedirs(SaveFolder)  \n",
    "    for i in range(len(ImageAndMasksList)):\n",
    "        obs_id = OBSID_List[i]\n",
    "        np.save(SaveFolder+obs_id+'.npy', ImageAndMasksList[i])\n",
    "else:\n",
    "    t0 = time.time()\n",
    "    #assumes the first if branch above has already been run\n",
    "    metadata_df = io.get_meta_data()\n",
    "    ImageAndMasksList=[]\n",
    "    OBSID_List=[]\n",
    "    for index, row in metadata_df.iterrows():\n",
    "        FileName = row['OBSERVATION_ID']+'_RGB.NOMAP.JP2'\n",
    "        OBSID_List.append( row['OBSERVATION_ID'])\n",
    "        ImageAndMasksList.append(np.load(SaveFolder+row['OBSERVATION_ID']+'.npy'))\n",
    "    t1 = time.time()\n",
    "    print('time to load pre-converted images = ',t1-t0)  #takes less than 5 minutes\n",
    "        \n",
    "#prepare data for training models (create 3 scales for each, to speed up training. Uses lots of RAM)\n",
    "DoScaleAug=True\n",
    "if DoScaleAug == True:\n",
    "    t0 = time.time()\n",
    "    ImageAndMasksLists = CreateListOfThreeScales(ImageAndMasksList) \n",
    "    t1 = time.time()\n",
    "    print(\"time to createlist of three scales for scale augmentation = \",t1-t0) #takes 1.5-2 hours\n",
    "else:\n",
    "    ImageAndMasksLists=[ImageAndMasksList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#params\n",
    "num_classes=2\n",
    "num_channels_in_input=3\n",
    "base_filters=16\n",
    "patch_height = 512\n",
    "patch_width = 512\n",
    "init_lr = 0.03\n",
    "epochs = 80\n",
    "FirstEpochStep = 40\n",
    "SecondEpochStep = 30\n",
    "wd=1e-4\n",
    "batch_size = 5*strategy.num_replicas_in_sync\n",
    "\n",
    "#output paths\n",
    "TrainingResultsPath='../../Data/TrainingResults/'\n",
    "if not os.path.isdir(TrainingResultsPath):\n",
    "    os.mkdir(TrainingResultsPath)\n",
    "TrainedModelsPath='../../Data/Models/SemanticSegmenterCNN/'\n",
    "if not os.path.isdir(TrainedModelsPath):\n",
    "    os.mkdir(TrainedModelsPath)\n",
    "\n",
    "#learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    lr = init_lr\n",
    "    if epoch >= SecondEpochStep:\n",
    "        lr *= 1e-2\n",
    "    elif epoch >= FirstEpochStep:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "#add some missing region names\n",
    "metadata_df = io.get_meta_data()\n",
    "region_names_df = io.get_region_names()\n",
    "region_names_df = region_names_df.set_index('obsid')\n",
    "region_names_df.at['ESP_012620_0975','roi_name'] = 'Buffalo'\n",
    "region_names_df.at['ESP_012277_0975','roi_name'] = 'Buffalo'\n",
    "region_names_df.at['ESP_012348_0975','roi_name'] = 'Taichung'\n",
    "for index, row in metadata_df.iterrows():\n",
    "    roi_name = region_names_df.at[row['OBSERVATION_ID'],'roi_name']\n",
    "    metadata_df.at[index,'roi_name']=roi_name\n",
    "\n",
    "#loop over each region, and leave it out of training for a model. Use the left out region for val\n",
    "UniqueP4Regions = metadata_df['roi_name'].unique()\n",
    "ModelCount = 0\n",
    "for ToLeaveOut in UniqueP4Regions:\n",
    "    #select data for training\n",
    "    LeaveOneRegionOutList = [n for n in UniqueP4Regions if n != ToLeaveOut]\n",
    "    print('Training model ',str(ModelCount+1),' of ',str(len(UniqueP4Regions)),'; leaving out region: ',ToLeaveOut)\n",
    "    TrainImages_df = metadata_df[metadata_df['roi_name'] != ToLeaveOut]\n",
    "    if DoScaleAug==True:\n",
    "        TrainImagesList1 = [ImageAndMasksLists[0][i] for i in  TrainImages_df.index.values]\n",
    "        TrainImagesList2 = [ImageAndMasksLists[1][i] for i in  TrainImages_df.index.values]\n",
    "        TrainImagesList3 = [ImageAndMasksLists[2][i] for i in  TrainImages_df.index.values]\n",
    "        TrainImagesList = [TrainImagesList1,TrainImagesList2,TrainImagesList3]\n",
    "    else:\n",
    "        TrainImagesList = [[ImageAndMasksLists[0][i] for i in  TrainImages_df.index.values]]\n",
    "    train_data_generator = P4_SegmenterDataGenerator(TrainImagesList,batch_size,patch_height,patch_width)\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    \n",
    "    #define the model\n",
    "    with strategy.scope():\n",
    "        model = model_hrnet_P4(num_channels_in_input,num_classes,base_filters,wd)\n",
    "        model.compile(loss=loss_jaccard_coef_categorical_one_minus,\n",
    "                  optimizer =SGD(lr=init_lr,decay=0, momentum=0.9, nesterov=False),\n",
    "                  metrics=[metric_precision,metric_recall,metric_jaccard_coef_categorical_int])\n",
    "    #fit the model\n",
    "    history = model.fit(train_data_generator,\n",
    "                              epochs=epochs,\n",
    "                              verbose=1, \n",
    "                              workers=4,\n",
    "                              callbacks=[lr_scheduler,History()],\n",
    "                              max_queue_size=4\n",
    "                             )\n",
    "    \n",
    "    #save outputs: model and training stats\n",
    "    TrainingInfo_df=pd.DataFrame.from_dict(model.history.history)\n",
    "    csv_file_name=TrainingResultsPath+'Training_HiRISE_segmenter_leave_out_' + ToLeaveOut+'_finished.csv'\n",
    "    TrainingInfo_df.to_csv(csv_file_name)\n",
    "\n",
    "    #save final model. Due to using parallel GPUs, much faster to save the weights and define a new model on CPU\n",
    "    temp_file_name=TrainedModelsPath+'HiRISE_segmenter_leave_out_'+ToLeaveOut+'final_model_weights_only_temp.h5'\n",
    "    model.save_weights(temp_file_name)\n",
    "    inference_model=model_hrnet_P4(num_channels_in_input,num_classes,base_filters,wd)\n",
    "    inference_model.load_weights(temp_file_name)\n",
    "    final_model_name=TrainedModelsPath+'HiRISE_segmenter_leave_out_'+ToLeaveOut+'final.h5'\n",
    "    inference_model.save(final_model_name,include_optimizer=False) \n",
    "    os.remove(temp_file_name)\n",
    "    \n",
    "    ModelCount=ModelCount+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
